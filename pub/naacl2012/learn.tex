\Section{learn}{Learning}
\Fig{fig/system}{0.40}{system}{
	An overview of the system architecture.
	Note that the parse in the middle is latent -- that is, it is not
		annotated in the training data.
}
We present a system architecture as described in \reffig{system}.
The inference procedure is described in \refsec{learn-model};
	training is detailed in \refsec{learn-train}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MODEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{learn-model}{Inference}
% -- K Best
To provide a list of
	candidate expressions with their associated probabilities, we
	employ a k-best CKY parser.
Specifically, we implement Algorithm 3 described in 
	\newcite{key:2005huang-parsing} -- providing an algorithm which is
	$O(k\log k)$ with respect to the beam size.
In our experiments, we set the beam size to 2000.

% -- Time Probabilities
Here, we revisit the notion of a temporal expression having an uncertainty
	over possible groundings, capturing a pragmatic ambiguity when
	referring to \ty{Sequence}s.
We represent this as a distribution with respect to the reference time.
In a sense, the most semantically complete output of the system would be
	such a distribution -- 
	an utterance of \tp{Friday}, instead of grounding to a 
	\textit{best guess} of what the time refers to, would rather give a 
	distribution over possible groundings along with their probabilities.
However, often it is advantageous to normalize to a concrete expression with a
	certain probability.

% -- Combination Method
The CKY k-best beam and the temporal distribution 
	-- capturing syntactic and pragmatic ambiguity --
	can be combined to provide a \textit{Viterbi} decoding, as well as
	its associated probability.
We define the probability of a syntactic parse $y$ making use of rules $R \in \sR$ 
	as $P(y) = P(w_1,\dots w_n; R) = \prod_{i \rightarrow j,k \in R}P(j,k \mid i)$.
	As described in \refsec{repr-types}, we define the probability of a 
	grounding relative to reference time $t$ and a particular
	syntactic interpretation as $P_t(i | y)$.
The product of these two terms provides the probability of a grounded temporal
	interpretation:

\begin{equation}
	P_t(i, y) = 
		P( y ) \times P_t(i | y)
\label{eqn:prob}
\end{equation}

% -- Viterbi
The Viterbi decoding is found by maximizing these two terms sequentially:
	the optimal grounding is found within the optimal syntactic parse.
This is in contrast to maximizing $P_t(i,R)$ directly, however, it avoids
	assigning a penalty to proposing ambiguous expressions (e.g., \te{Friday})
	versus unambiguous expressions (e.g., \te{today}).
%Furthermore, it is reasonable to assume that cognitively we perform a
%	similar procedure -- resolving syntactic ambiguity before resolving
%	pragmatic ambiguity.

% -- Segue
This provides us with a framework for obtaining grounded times from a
	temporal phrase -- in line with the annotations provided during training
	time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRAINING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{learn-train}{Training}
% -- Intro
%(catch phrase)
We present an EM-\textit{esque} bootstrapping approach to training the 
	parameters of our grammar jointly with the parameters of our 
	Gaussian temporal distribution.
%(table of contents)
The preprocessing steps are described below, followed by an overview
	of the training regime and finally the algorithm itself.

% -- Preprocessing
\paragraph{Preprocessing}
The input phrase is tokenized according to Penn Treebank guidelines,
	except we additionally split on the characters `-' and `/,' which
	often delimit a boundary between temporal entities.
Thereafter, numbers written in text are heuristically converted to their
	numeric values, and featurized according to their order of magnitude
	and ordinality.
For instance, the phrase \tp{three-day} would be preprocessed to create the
	tokens $\left<\right.$ \tp{NUM(0-9,non-ordinal)} , \tp{-} , \tp{day} 
		$\left.\right>$.

Beyond this preprocessing, no language-specific information about the meanings
	of the words are introduced, including syntactic parses, POS tags,
	\textit{et cetera}.


% -- Algorithm Figure
\begin{algorithm}[t]
	%(defines)
	\SetKw{Def}{def}
	\SetKwFunction{estep}{E-Step}
	\SetKwFunction{mstep}{M-Step}
	%(input/output)
	\KwIn{Initial parameters $\theta$, $\mu$, $\sigma$; 
		data $\sD=\left\{(w,\tau^*,t)\right\}$}
	\KwOut{Optimal parameters $\theta^*$, $\mu^*$, $\sigma^*$}
	\BlankLine

	%(main loop)
	\While{not converged}{
		$\bar M$ := \estep$(\sD$,$\theta$,$\mu$,$\sigma)$ \\
		$(\theta,\mu,\sigma)$ := \mstep$(\bar M)$
	}
	\KwRet $(\theta_s,\mu,\sigma)$ \\
	\BlankLine
	\BlankLine

	%(E-Step)
	\Begin(\estep{$\theta$,$\mu$,$\sigma$}){
		\For{$(w,\tau^*,t) \in \sD$}{
			$\bar m$ = emptyESS() \\
			\For{$y \in$ k-bestCKY$(w,\theta)$}{ \label{line:parsebeam}
				\For{$\tau_i \in$ groundings$(y,\mu,\sigma)$}{ \label{line:timebeam}
					\If{$\tau_i = \tau^*$ \texttt{and} ok(y,i)}{ \label{line:validity}
						$\bar m_{\theta}$ += count($y$,$P_t(i,y)$) \\
						$\bar m_{\mu,\sigma}$ += $\left(i,P_t(i,y)\right)$ \\
						$Z$ += $P_t(i,y)$ \\
					}
				}
			}
			$\bar M$ += normalize$(\bar m_{\theta}, \bar m_{\mu,\sigma}, Z)$ \\
		}
		\KwRet $\bar M$ \\
	}
%	\BlankLine
%	\BlankLine
%	\Begin(\mstep{$\hat M$}){
%		\tcc{Standard M step}
%	}

%	%(algorithm)
%	\SetKw{Def}{def}
%	\SetKwFunction{generateCandidates}{generateCandidates}
%	\SetKwFunction{updateParameters}{updateParameters}
%	%(input/output)
%	\KwIn{Initial parameters $\theta$, $\mu$, $\sigma$; 
%		data $\sD=\left\{(w,\tau^*,t)\right\}$}
%	\KwOut{Optimal parameters $\theta^*$, $\mu^*$, $\sigma^*$}
%	%(main loop)
%%	\tcc{Iterative parameter update}
%	$(\theta',\mu',\sigma')$ := $(\theta_s,\mu,\sigma)$ \\
%	\While{not converged}{
%		$\sH$ := generateCandidates$(\theta_s',\mu',\sigma')$ \\
%%		$(\theta_s',\mu',\sigma')$ = updateParameters$(\sH)$
%		$(\theta_s',\mu',\sigma')$ = fullyObservedEM$(\sH)$
%	}
%	\KwRet $(\theta_s',\mu',\sigma')$ \\
%
%	\BlankLine
%	\BlankLine
%	%(generate candidates)
%%	\tcc{Generate candidate valid parses}
%	\Begin(\generateCandidates{$\theta_s$,$\mu$,$\sigma$}:){
%		\sH := $\left\{\right\}$ \\
%		\For{$(w,\tau^*,t) \in \sD$}{
%			Z := 0; H = $\left\{\right\}$ \\
%			\For{$(R,\tau_t,P_t) \in$ k-bestCKY(w)}{ \label{line:parsebeam}
%				\For{$i \in i_{\textrm{max}}-2 \dots i_{\textrm{max}}+2$}{ \label{line:timebeam}
%					\If{$\tau(i) = \tau^*$ \texttt{and} ok(R,i)}{ \label{line:validity}
%						H += $(R,\tau,i,P_t(i))$ \\
%						Z += $P_t(i)$
%					}
%				}
%			}
%			\For{$(R,\tau,i,p) \in H$}{ 
%				\sH += $(R,\tau,i,p/Z)$ 
%			}
%		}
%		\Return{\sH}
%	}
	%(caption)
	\caption{
		\label{alg:pseudocode}
		TimEM 
	}
\end{algorithm}

% -- Algorithm
\paragraph{Algorithm}
%(algorithm reference)
Our TimEM algorithm for learning the parameters for the grammar ($\theta$),
	jointly with the temporal distribution ($\mu$ and $\sigma$) is given 
	in \refalg{pseudocode}.
%(input/output)
The inputs to the algorithm are the initial parameters $\theta$,$\mu$, and
	$\sigma$, and a set of training instances $\sD$.
Each training instance is a tuple consisting of the words in the temporal
	phrase $w$, the annotated grounded time $\tau^*$, and the reference time
	of the utterance $t$.
The algorithm outputs the final parameters $\theta^*$, $\mu^*$ and $\sigma^*$.

%(EM comparison)
The algorithm operates in an EM-\textit{esque} fashion, similar in style
	to algorithms used in the grammar induction
	literature \cite{key:2004klein-induction,key:1992carroll-induction}.
However, unlike grammar induction, we are given loose supervision giving a 
	necessary but not sufficient condition for a parse being a valid completion.
Line \ref{line:validity} in \refalg{pseudocode} accounts for this filtering.
Our \textit{expected counts} are therefore more accurately our normalized
	expected counts of \textit{valid} parses.

%(beams)
Additionally, the expected statistics are computed non-analytically by use of
	a beam on both the possible parses (line \ref{line:parsebeam}) and the
	possible temporal groundings of a given interpretation (line
	\ref{line:timebeam}).

%(em updates)
Given this artificially completed data, the EM updates are the standard
	updates for multinomial and Gaussian distributions given fully observed data.
In the multinomial case, our parameter updates are:
\begin{equation}
	\theta_{mn \mid l}' =
		\sum\limits_{(R,\tau,i,p) \in \sH} \sum\limits_{v_{jk \mid i} \in R}
		\frac{
			\1(v_{jk \mid i} = v_{mn \mid l}) \times p
		}{
			\bZ
		}
\label{eqn:multinomialEM}
\end{equation}

In the Gaussian case, the parameter updates are:
\begin{align}
	\mu' =&
		\frac{ 1 } { \sum\limits_{(R,\tau,i,p) \in \sH}  p }
		\sum\limits_{(R,\tau,i,p) \in \sH} i \times p \\
	\sigma' =& \sqrt{
		\frac{ 1 } { \sum\limits_{(R,\tau,i,p) \in \sH}  p }
		\sum\limits_{(R,\tau,i,p) \in \sH} (i-\mu')^2 \times p
	}
\label{eqn:multinomialEM}
\end{align}

%(big picture)
As the parameters converge to their optimal values, the beam incorporates
	valid parses for longer and longer phrases.
In this way, a progressively larger percentage of the data is available to be
	learned from at each iteration.

%(tweaks)
To assist in this process, and taking insight from the Gricean 
	maxims of quantity and quality, we further restrict the set of valid 
	parses to include only those which are as short as possible
	(that is, maximally \ty{Nil}-padded) and which are pragmatically clear.
That is -- with notable but practically relatively rare exceptions --
	more often than not we do not repeat ourselves when describing time.
Apart from phrases such as \tp{last Friday the \th{13}} where \tp{last Friday}
	and \tp{the \th{13}} refer to the same time, we rarely describe a time as
	\tp{today this week} or \tp{this month Friday}, \textit{et cetera}.
It's important to note that the model still handles these cases, only it
	disprefers them during training.

To balance the tendency to use short phrases, we observe that
	temporal phrases are described as pragmatically unambiguously as possible.
That is, one is more likely to refer to the previous Friday as \tp{last Friday}
	than simply \tp{Friday}, though the former is somewhat longer.
To capture this, we take candidate interpretations which ground at the
	temporal expression's origin if possible --
	in our example we would take \tp{last Friday} but not \tp{Friday}.

These are encapsulated in the \textit{ok()} function
	on Line \ref{line:validity} of \refalg{pseudocode}.

