\Section{learn}{Learning}
\Fig{fig/system}{0.40}{system}{
	An overview of the system architecture.
	Note that the parse is latent -- that is, it is not
		annotated in the training data.
}
We present a system architecture, described in \reffig{system}.
We detail the inference procedure in \refsec{learn-model}
	and training in \refsec{learn-train}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MODEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{learn-model}{Inference}
% -- K Best
To provide a list of
	candidate expressions with their associated probabilities, we
	employ a $k$-best CKY parser.
Specifically, we implement Algorithm 3 described in 
	\newcite{key:2005huang-parsing}, providing an $O(Gn^3k\log k)$ algorithm
	with respect to the grammar size $G$, phrase length $n$, and beam size $k$.
We set the beam size to 2000.

% -- Time Probabilities
Revisiting the notion of pragmatic ambiguity,
	in a sense the most semantically complete output of the system would be
	a distribution -- 
	an utterance of \tp{Friday} would give a distribution over Fridays
	rather than a best guess of its grounding.
However, it is often advantageous to ground to a concrete expression with a
	corresponding probability.

% -- Combination Method
The CKY $k$-best beam and the temporal distribution 
	-- capturing syntactic and pragmatic ambiguity --
	can be combined to provide a \textit{Viterbi} decoding, as well as
	its associated probability.
We define the probability of a syntactic parse $y$ making use of rules 
	$R \subseteq \sR$ as
	$P(y) = P(w_1,\dots w_n; R) = \prod_{i \rightarrow j,k \in R}P(j,k \mid i)$.
	As described in \refsec{repr-types}, we define the probability of a 
	grounding relative to reference time $t$ and a particular
	syntactic interpretation as $P_t(i | y)$.
The product of these two terms provides the probability of a grounded temporal
	interpretation; we can obtain a Viterbi decoding by maximizing this
	joint probability:

\begin{equation}
	P_t(i, y) = 
		P( y ) \times P_t(i | y)
\label{eqn:prob}
\end{equation}

% -- Viterbi %TODO which do we use?
%The Viterbi decoding is found by maximizing these two terms sequentially:
%	the optimal grounding is found within the optimal syntactic parse.
%This is in contrast to maximizing $P_t(i,y)$ directly, but avoids
%	assigning a penalty for proposing ambiguous expressions (e.g., \te{Friday})
%	versus unambiguous expressions (e.g., \te{today}).

% -- Segue
This provides us with a framework for obtaining grounded times from a
	temporal phrase -- in line with the annotations provided during training
	time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRAINING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{learn-train}{Training}
% -- Intro
%(catch phrase)
We present an EM-style bootstrapping approach to training the 
	parameters of our grammar jointly with the parameters of our 
	Gaussian temporal distribution.
%%(table of contents)
%The preprocessing steps are described below, followed by an overview
%	of the training regime and finally the algorithm itself.

%% -- Preprocessing
%\paragraph{Preprocessing}
%The input phrase is tokenized according to Penn Treebank guidelines,
%	except we additionally split on the characters `-' and `/,' which
%	often delimit a boundary between temporal entities.
%%Thereafter, for the type grammar, 
%%	numbers written in text are heuristically converted to their
%%	numeric values and featurized according to their order of magnitude
%%	and ordinality.
%%For instance, the phrase \tp{three-day} would be preprocessed to create the
%%	tokens $\left<\right.$ \tp{NUM(0-9,non-ordinal)} , \tp{-} , \tp{day} 
%%		$\left.\right>$.
%Beyond this preprocessing, no language-specific information about the meanings
%	of the words are introduced, including syntactic parses, POS tags, etc.


% -- Algorithm Figure
\begin{algorithm}[t]
	%(defines)
	\SetKw{Def}{def}
	\SetKwFunction{estep}{E-Step}
	\SetKwFunction{mstep}{M-Step}
	%(input/output)
	\KwIn{Initial parameters $\theta$, $\mu$, $\sigma$; 
		data $\sD=\left\{(w,\tau^*,t)\right\}$; 
		Dirichlet prior $\alpha$, Gaussian prior $\sN$
	}
	\KwOut{Optimal parameters $\theta^*$, $\mu^*$, $\sigma^*$}
	\BlankLine

	%(main loop)
	\While{not converged}{
		$(\bar M_\theta, \bar M_{\mu,\sigma})$ := \estep$(\sD$,$\theta$,$\mu$,$\sigma)$ \\
		$(\theta,\mu,\sigma)$ := \mstep$(\bar M_\theta, \bar M_{\mu,\sigma})$
	}
	\KwRet $(\theta_s,\mu,\sigma)$ \\
	\BlankLine
	\BlankLine

	%(E-Step)
	\Begin(\estep{$\sD$,$\theta$,$\mu$,$\sigma$}){
		$\bar M_\theta = []$; $\bar M_{\mu,\sigma} = []$ \\
		\For{$(w,\tau^*,t) \in \sD$}{
			$\bar m_\theta$ = []; $\bar m_{\mu,\sigma}$ = [] \\
			\For{$y \in$ k-bestCKY$(w,\theta)$}{ \label{line:parsebeam}
				\If{$p = P_{\mu,\sigma}(\tau^* \mid y, t) > 0$}{ \label{line:timebeam}
						$\bar m_{\theta}$ += $\left(y,p\right)$;
							$\bar m_{\mu,\sigma}$ += $\left(i,p\right)$ \\
				}
			}
			$\bar M$ += normalize$(\bar m_{\theta})$ \\
			$\bar M_{\mu,\sigma}$ += normalize$(\bar m_{\mu,\sigma})$ \\
		}
		\KwRet $\bar M$ \\
	}
	\BlankLine
	\BlankLine
	
	%(M-Step)
	\Begin(\mstep{$\bar M_\theta$,$\bar M_{\mu,\sigma}$}){
		$\theta'$ := bayesianPosterior($\bar M_\theta$, $\alpha$) \\
		$\sigma'$ := mlePosterior($\bar M_{\mu,\sigma}$) \\
		$\mu'$ := bayesianPosterior($\bar M_{\mu,\sigma}$, $\sigma'$, $\sN$) \\
		\KwRet ($\theta'$, $\mu'$, $\sigma'$) \\
	}
	%(caption)
	\caption{
		\label{alg:pseudocode}
		TimEM 
	}
\end{algorithm}

% -- Algorithm
%\paragraph{Algorithm}
%(algorithm reference)
Our TimEM algorithm for learning the parameters for the grammar ($\theta$),
	jointly with the temporal distribution ($\mu$ and $\sigma$) is given 
	in \refalg{pseudocode}.
%(input/output)
The inputs to the algorithm are the initial parameters $\theta$,$\mu$, and
	$\sigma$, and a set of training instances $\sD$.
Furthermore, the algorithm makes use of a Dirichlet prior $\alpha$ on 
	the grammar parameters $\theta$, as well as a Gaussian prior $\sN$ on the
	mean of the temporal distribution $\mu$.
The algorithm outputs the final parameters $\theta^*$, $\mu^*$ and $\sigma^*$.


Each training instance is a tuple consisting of the words in the temporal
	phrase $w$, the annotated grounded time $\tau^*$, and the reference time
	of the utterance $t$.
The input phrase is tokenized according to Penn Treebank guidelines,
	except we additionally split on the characters `-' and `/,' which
	often delimit a boundary between temporal entities.
Beyond this preprocessing, no language-specific information about the meanings
	of the words are introduced, including syntactic parses, POS tags, etc.

%(EM comparison)
The algorithm operates similar to the EM algorithms used for grammar induction
	\cite{key:2004klein-induction,key:1992carroll-induction}.
However, unlike grammar induction, we are allowed a certain amount of
	supervision by requiring that the predicted temporal expression
	match the annotation.
Our \textit{expected statistics} are therefore more accurately our normalized
	expected counts of \textit{valid} parses.

%(can't read off expectation analytically)
Note that in conventional grammar induction, the expected sufficient statistics
	can be gathered analytically from reading off the chart scores of a parse.
This does not work in our case for two reasons.
In part, we would like to incorporate the probability of the temporal
	grounding in our feedback probability.
Additionally, we are only using parses which are valid candidates -- that is,
	which ground to the correct time $\tau*$ -- which we cannot establish
	until the entire expression is parsed.
%(beams)
The expected statistics are thus computed non-analytically by use of
	a beam on both the possible parses (line \ref{line:parsebeam}) and the
	possible temporal groundings of a given interpretation (line
	\ref{line:timebeam}).

%(em updates)
The particular EM updates are the standard
	updates for multinomial and Gaussian distributions given fully observed data.
In the multinomial case, our (unnormalized) parameter updates, with
	Dirichlet prior $\alpha$, are:

\begin{equation}
	\theta_{mn \mid l}' =
		\alpha + 
		\sum\limits_{(y,p) \in \bar M_\theta} \sum\limits_{v_{jk \mid i} \in y}
			\mathbbm{1}\left(v_{jk \mid i} = v_{mn \mid l}\right) p
\label{eqn:multinomialEM}
\end{equation}

In the Gaussian case, the parameter update for $\sigma$ is the maximum
	likelihood update; while the update for $\mu$ incorporates a
	Bayesian prior $sN(\mu_0,\sigma_0)$:

\begin{align}
	\sigma' =& \sqrt{
		\frac{ 1 } { \sum\limits_{(i,p) \in \bar M_{\mu,\sigma}}  p }
		\sum\limits_{(i,p) \in \bar M_{\mu,\sigma}} (i-\mu')^2 \cdot p
	} \\
	\mu' =&
		\frac{
			\sigma^2 \mu_0 + \sigma_0^2 \sum_{(i,p) \in \bar M_{\mu,\sigma}}  i \cdot p
		}{
			\sigma^2 + \sigma_0^2 \sum_{(i,p) \in \bar M_{\mu,\sigma}} p 
		}
\label{eqn:gaussianEM}
\end{align}

%(big picture)
As the parameters improve, the parser more efficiently prunes incorrect parses
	and the beam incorporates valid parses for longer and longer phrases.
For instance, in the first iteration the model must learn the meaning of both
	words in \tp{last Friday}; once the parser learns the meaning of one of them
	(e.g., \tp{Friday} appears elsewhere in the corpus) subsequent iterations
	focus on proposing candidate meanings for \tp{last}.
In this way, a progressively larger percentage of the data is available to be
	learned from at each iteration.

%%(tweaks)
%To assist in this process, and taking insight from the Gricean 
%	maxims of quantity and quality, we further restrict the set of valid 
%	parses to include only those which are as concise as possible
%	(that is, maximally \ty{Nil}-padded on both sides) 
%	and which are pragmatically clear.
%That is -- with notable but practically relatively rare exceptions --
%	more often than not we do not repeat ourselves when describing time.
%Apart from phrases such as \tp{last Friday the \th{13}}, where \tp{last Friday}
%	and \tp{the \th{13}} refer to the same time, we rarely describe a time as
%	\tp{today this week} or \tp{this month Friday}, etc.
%It's important to note that the model still handles these cases, only it
%	disprefers them during training.
%
%To balance the tendency to use short phrases, we observe that
%	temporal phrases are described as pragmatically unambiguously as possible.
%That is, one is more likely to refer to the previous Friday as \tp{last Friday}
%	than simply \tp{Friday}, though the former is somewhat longer.
%To capture this, we take candidate interpretations which ground at the
%	temporal expression's origin if possible --
%	in our example we would take \tp{last Friday} but not \tp{Friday}.
%
%These are encapsulated in the \textit{ok()} function
%	on Line \ref{line:validity} of \refalg{pseudocode}.

