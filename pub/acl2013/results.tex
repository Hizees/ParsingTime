\Section{results}{Evaluation}

% -- Result English
\begin{table}
	\begin{center}
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		       & \multicolumn{2}{c|}{Train} & \multicolumn{2}{c|}{Test} \\
		System & Type & Value  & Type & Value \\
		\hline
		\sys{GUTime}      & 0.72          & 0.46          & 0.80           & 0.42 \\
		\sys{SUTime}      & 0.85          & 0.69          & \textbf{0.94}  & 0.71 \\
		\sys{HeidelTime}  & 0.80          & 0.67          & 0.85           & 0.71 \\
		\sys{ParsingTime} & 0.90          & 0.72          & 0.88           & 0.72 \\
		\hline                                           
		\sys{OurSystem}   & \textbf{0.94} & \textbf{0.81} & 0.91           & \textbf{0.76} \\
		\hline
	\end{tabular}
	%(caption)
	\caption{
		English results for \tempeval\ attribute scores for our system and
      four previous systems.
		The scores are calculated using gold extents, forcing an
		interpretation for each parse.
		\label{tab:results-english}
	}
	\end{center}
\end{table}

% -- Result Spanish
\begin{table}
	\begin{center}
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		       & \multicolumn{2}{c|}{Train} & \multicolumn{2}{c|}{Test} \\
		System & Type & Value  & Type & Value \\
		\hline
		\sys{UC3M}        & ---           & ---           & 0.79           & 0.72 \\
		\hline                                           
		\sys{OurSystem}   & 0.90          & 0.84          & \textbf{0.92}  & \textbf{0.76} \\
		\hline
	\end{tabular}
	%(caption)
	\caption{
		Spanish results for \tempeval\ attribute scores for our system and
      the best known previous system.
		The scores are calculated using gold extents, forcing an
		interpretation for each parse.
		\label{tab:results-spanish}
	}
	\end{center}
\end{table}

% -- Intro
We evaluate our model on all six languages in the \tempeval\ Task A dataset
  \cite{key:2010verhagen-tempeval}, comparing against state-of-the-art
  systems for English and Spanish.
New results are reported on smaller data from the four other languages.
To our knowledge, there has not been any prior work on these new corpora.

% -- Outline
We describe the \tempeval\ datasets in \refsec{tempeval} and experimental
  results in \refsec{scores}.

% -----
% DATASET
% -----
\Subsection{tempeval}{\tempeval\ Datasets}

% -- Result All
\begin{table*}[ht]
	\begin{center}
	\begin{tabular}{|l|c|c|c|c|c|c|}
		\hline
		       & \multicolumn{3}{c|}{Train} & \multicolumn{3}{c|}{Test} \\
		Language & \# examples & Type & Value  & \# examples & Type & Value \\
		\hline
		English                        & 1052 & 0.94 & 0.81  & 156 & 0.91 & 0.76 \\
		Spanish                        & 1092 & 0.90 & 0.84  & 198 & 0.92 & 0.76 \\
		Italian                        & 523  & 0.89 & 0.85  & 126 & 0.84 & 0.38 \\
		Chinese$^\dagger$              & 744  & 0.95 & 0.65  & 190 & 0.87 & 0.48 \\
		Chinese (clean)$^\dagger$      & 659  & 0.97 & 0.73  & 143 & 0.97 & 0.60 \\
		Korean$^\dagger$               & 247  & 0.83 & 0.67  & 91  & 0.82 & 0.42 \\
		French$^\dagger$               & 206  & 0.78 & 0.76  & 83  & 0.78 & 0.35 \\
		\hline
	\end{tabular}
	%(caption)
	\caption{
    Our system's accuracy on all 6 languages of the \tempeval\ corpus.
    Chinese is divided into two results: one for the entire corpus, and one
      which considers only examples for which a temporal value is annotated.
    Languages with a dagger ($^\dagger$) were evaluated based on semantic
      rather than string-match correctness.
		\label{tab:results-all}
	}
	\end{center}
\end{table*}

% -- Intro
\tempeval, from SemEval 2010, focused on retrieving and reasoning
  about temporal information from newswire.
Our system evaluates against Task A -- detecting and resolving temporal
  expressions.
Since we perform only the second of these, we evaluate our system
	assuming gold detection.

The dataset annotates six languages:
  English, Spanish, Italian, French, Chinese, and Korean;
  of these, English and Spanish are the most mature.
We describe each of these languages, along with relevant quirks, below:

\paragraph{English}
% -- English
% (general)
The English dataset consists of 1052 training examples, and 156 test examples.
% (evaluation)
Evaluation was done using the official evaluation script, which checks for
  exact match between \timex\ tags.
Note that this is a restrictive metric; for instance, \tp{24 hours} and
  \tp{a days} have the same interpretation, but
  have different \timex\ strings.
System output was heuristically converted to the \timex\ format; where
  ambiguities arose, the convention which maximized training accuracy was
  chosen.
Empirically, the error introduced in this conversion was less than 1\%.

\paragraph{Spanish}
% -- Spanish
The Spanish dataset consists of 1092 training examples, and 198 test examples.
Evaluation was identical to the English, with the heuristic \timex\ conversion
  adapted somewhat.

\paragraph{Italian}
% -- Italian
% (general)
The Italian dataset consists of 523 training examples, and 126 test examples.
Evaluation was identical to English and Spanish.

\paragraph{Chinese}
% -- Chinese
% (general)
The Chinese dataset consists of 744 training examples, and 190 test examples.
Of these, only 659 training and 143 test examples had a temporal value marked;
  the remaining examples had a type but no value, and are therefore
  impossible to predict.
Results are also reported on a clean corpus with these impossible examples
  omitted.

% (evaluation)
The Chinese, Korean, and French corpora had significantly more severe
  inconsistencies in the \timex\ annotation.
Thus, evaluations are reported according to the training objective: if two
  \timex\ values ground to the same grounded time, they are considered equal.
For example, in the example above, \textit{24 hours} and \textit{a day}
  would be marked identical despite having different \timex\ strings.

% (conversion subtlety)
Most \timex\ values convert naturally to a grounded representation; values
  with wildcards representing Sequences (e.g., \te{1998-QX} or \te{1998-XX-12})
  ground to the same value as the \ty{Sequence} encoding that value would.
For instance, \te{1998-QX} would be parsed as \tp{every quarter in 1998}.

\paragraph{Korean}
% -- Korean
% (general)
The Korean dataset consists of 287 training examples, and 91 test examples.
% (garbage in)
40 of the training examples encoded dates as a long integer
For example: 003000000200001131951006 grounds to January 13, 2000 at the time
  19:51.
These were removed from the training set, yielding 247 examples; however,
  all three such examples were left in the test set.
% (evaluation)
Evaluation was done identically to the Chinese data.

\paragraph{French}
% -- French
% (general)
Lastly, a dataset for French temporal expressions was compiled from the
  \tempeval\ data.
% (scraping)
Unlike the other 5 languages, the French data included only the raw 
  \timex\ annotated newswire documents, encoded as XML.
These documents were scraped to recover 206 training examples and 83 test
  examples.
% (evaluation)
Evaluation was done identically to the Chinese and Korean data.

% -- Segway
We proceed to describe our experimental results on these datasets.

% -----
% SCORES
% -----
\Subsection{scores}{Results}
% -- Other Systems
We compare our system with state-of-the-art systems for both English and
  Spanish.
To the best of our knowledge, no prior work exists for the other four languages.

% -- Evaluation methodology
We evaluate in the same framework as \me.
We compare to 
	previous system scores when constrained to make a prediction on every
	example; if no guess is made, the output is considered incorrect.
This in general yields lower results for those systems,
  as the system is not allowed to
	abstain on expressions it does not recognize.

% -- Comparisons
The systems compared against are:
\begin{itemize}
\setlength{\itemsep}{0cm}
\item GUTime \cite{key:2000mani-temporal}, a widely used, older rule-based
              system.
\item HeidelTime \cite{key:2010strotgen-temporal}, the top
                 system at the \tempeval\ task for English.
\item SUTime \cite{key:2012chang-temporal}, a more recent rule-based
             system for English.
\item ParsingTime \mec, a semantic parser for temporal expressions, similar to
                  this system (see \refsec{related}).
\item UC3M \cite{2010vicente-uc3m}, a rule-based system for
             Spanish.
\end{itemize}

% -- Results
Results for the English corpus are shown in \reftab{results-english}.
Results for Spanish are shown in \reftab{results-spanish}.
Lastly, a summary of results in all six languages is shown in
  \reftab{results-all}.

% -- Analysis
% (overfitting)
A salient trend emerges from the results -- while training accuracy is
  consistently high, test accuracy drops sharply for the data-impoverished
  languages.
This is consistent with what would be expected from a discriminatively trained
  model in data-impoverished settings; however, the consistent training accuracy
  suggests that the model nonetheless captures the phenomena it sees
  in training well.
% (can fit more?)
This suggests the possibility for improving accuracy further by making use of
  more data during training.


% -----
% DISCUSSION
% -----
\Subsection{discuss}{Discussion}
We characterize the examples our system parses incorrectly
  on the English and Spanish datasets in
  \reftab{mistakes}, expanding on each class of error below.

\begin{table}[ht]
	\begin{center}
	\begin{tabular}{|l|c|c|}
  \hline
  \textbf{Error Class} & \textbf{English} & \textbf{Spanish} \\
  \hline
  Pragmatics            & $29\%$ & $23\%$ \\
  Type error            & $16\%$ & $5\%$ \\
  Incorrect number      & $10\%$ & $5\%$  \\
  Relative \ty{Range}   & $7\%$  & $2\%$ \\
  Incorrect parse       & $19\%$ & $36\%$ \\
  \hline
  Missing context       & $16\%$  & $23\%$ \\
  Bad reference time    & $3\%$  & $6\%$ \\
  \hline
	\end{tabular}
	%(caption)
	\caption{
    A summary of errors of our system, by percentage of incorrect examples
      for the English and Spanish datasets.
    The top section describes errors which we could be fixed in our framework,
      while the bottom section describes examples which are either ambiguous
      (missing context), or appear incorrect in isolation relative to the
      reference time.
		\label{tab:mistakes}
	}
	\end{center}
\end{table}

\paragraph{Pragmatics}
This class of errors is a result of pragmatic ambiguity over possible groundings
  of a sequence -- for instance, it is often
  ambiguous whether \tp{next weekend} refers to the
  coming or subsequent weekend.
These errors manifest in either dropping a function (\tp{next}, \tp{last}),
  or imagining one that is not supported by the text (e.g., \tp{this week}
  parsed as next week).

\paragraph{Type error}
Another large class of errors -- particularly in the English dataset --
  arise from not matching the annotation's type, but otherwise producing
  a reasonable response.
For instance, the system may mistake a day on the calendar (a \ty{Range}),
  with a day, the period of time.

\paragraph{Incorrect number}
A class of mistakes arise from either omitting numbers from the parse,
  or incorrectly parsing numbers -- the second case is particularly prevalent
  for written years, such as \tp{seventeen seventy-six}.

\paragraph{Relative \ty{Range}}
These errors arise from attempting to parse a grounded \ty{Range} by applying
  functions to the reference time.
For example, from a reference time of August \th{8}, you could ``correctly''
  parse the phrase \tp{August 1} as a week ago; but, naturally, this parse does
  not generalize well.
This class of errors, although relatively small, merits special designation
  as it suggests a class of correct responses which are correct for the wrong
  reasons.
Future work could explore mitigating these errors for domains
  where the text is further removed from the events it is describing
  than most news stories are.

\paragraph{Incorrect parse}
Errors in this class are a result of failing to find the correct parse,
  for a number of reasons not individually identified.
A small but notable subset of these errors ($6\%$ on the Spanish dataset)
  are a result of the grammar
  being insufficiently expressive (e.g., we cannot capture fractional
  units -- \tp{half an hour}).

\paragraph{Missing context}
A fairly large percentage of our errors arise from failing to classify inputs
  which express ambiguous or poorly defined times.
For example, \tp{from time to time} (annotated as the future), or \tp{that time}
  (annotated as 5 years).
Many of these require a broader understanding of the context in which the
  temporal phrase is uttered, which our system does not attempt to capture.

\paragraph{Bad reference time}
The last class of errors cover cases where the temporal phrase is clear,
  but annotation differs from our judgment of what would be reasonable.
These are a result of assuming that the reference time of an utterance is
  the publication time of the article.
