\Section{learn}{Learning}
The system is trained using a discriminative $k$-best parser, which is able to
  incorporate arbitrary features over partial derivations.
We describe the parser below, followed by the features implemented.

% -----
% PARSER
% -----
\Subsection{parser}{Parser}
\paragraph{Inference}
% -- Overview
A discriminative $k$-best parser was used to allow for arbitrary features
  in the parse tree.
In the first stage, spans of the input sentence are tagged as either text
  or numbers.
A rule-based number recognizer was used for each language to recognize and
  ground numeric expressions, including information on whether the number was
  an ordinal (e.g., \tp{two} versus \tp{second}).
Note that unlike conventional parsing, a tag can span multiple words.
Numeric expressions are treated as if the numeric value replaced the
  expression.

Each rule of the parse derivation was assigned a score according to a log-linear
  factor.
Specifically, each rule $R = (v_i \rightarrow v_j v_k, f)$
  with features $\phi(R)$ subject to parameters
  $\theta$ is given a probability:

\begin{equation}
P(v_i \mid v_j, v_k, f; \theta) \propto e^{ \theta^T \phi(R) }
\end{equation}

Na\"{\i}vely, this parsing algorithm gives us a complexity of $O(n^3 k^2)$,
  where $n$ is the length of the sentence, and $k$ is the size of the beam.
However, we can approximate the algorithm in $O(n^3 k \log k)$ time with
  cube pruning \cite{key:2007chiang-cubepruning}.
With features which are not context-free, we are not
  guaranteed an optimal beam with this approach; however, empirically
  the approximation yields a significant efficiency improvement without
  noticeable loss in performance.

\paragraph{Training}
We adopt an EM-style bootstrapping approach similar to \me\ in order to handle
  the task of parsing the temporal expression without 
  annotations for the latent parses.
Each training instance is a tuple consisting of the words in the temporal
	phrase, the annotated grounded time $\tau^*$, and the reference time.
Given an input sentence, our parser will output $k$ possible parses; when
  grounded to the reference time these correspond to $k$ candidate times:
  $\tau_1 \dots \tau_k$, each with a normalized probability $P(\tau_i)$.

This corresponds to an approximate E step in the EM algorithm, where the
  distribution over latent parses is approximated by a beam of size $k$.
Although for long sentences the number of parses is far greater than the
  beam size, as the parameters improve, increasingly longer sentences will
  have correct derivations in the beam.
In this way, a progressively larger percentage of the data is available to be
	learned from at each iteration.

To approximate the M step, 
  we define a multi-class hinge loss $l(\theta)$ over the beam, and
  optimize using Stochastic Gradient Descent with AdaGrad
  \cite{key:2010duchi-adagrad}:

\begin{equation}
l(\theta) = 
  \max_{0 \leq i < k} \1[\tau_i = \tau^*] + P_\theta(\tau_i)
                                                 - P_\theta(\tau^*)
\end{equation}

We proceed to describe the features used in the parser.

% -----
% FEATURES
% -----
\FigStar{fig/system.pdf}{0.40}{features}{
  An example parse of \tp{Friday of this week}, along with the features
    extracted from the parse.
  A summary of the input, latent parse, and output for a particular example
    is given in (a).
  The features extracted for each fragment of the parse are given in (b).
}

\Subsection{features}{Features}
Our framework allows us to define arbitrary features over partial derivations.
Importantly, this allows us to condition not only on the PCFG probabilities
  over \textit{types} but also the partial semantics of the
  derivation.
We describe the features used below; a summary of these features
  for a short phrase is illustrated in \reffig{features}.

\paragraph{Bracketing Features}
% -- Introduction
% (definition)
A feature is defined over every nonterminal combination,
  consisting of the pair of children being combined in that rule.
In particular, let us consider a rule
  \mbox{$R = (v_i \rightarrow v_j v_k, f)$} corresponding to a CFG rule
  \mbox{$v_i \rightarrow v_j v_k$} over \textit{types} and a function $f$
  over the semantic values corresponding to 
  $v_j$ and $v_k$: $\tau_j$ and $\tau_k$.
% (two classes)
Two classes of bracketing features are extracted:
  features are extracted over the types of nonterminals being combined
    ($v_j$ and $v_k$),
  and over the top-level semantic derivation of the nonterminals
    ($f$, $\tau_j$, and $\tau_k$).

% -- Type Bracketing
Unlike syntactic parsing, child types of a parse tree
  uniquely define the parent type of the rule; this is a direct consequence
  of our combination rules being functions with domains defined in terms of the
  temporal types, and therefore necessarily projecting
  their inputs into a single output type.
Therefore, the first class of bracketing features -- over
  types -- reduce to have the exact same
  expressive power as the nonterminal CFG rules of \me.

% -- Value Bracketing
However, we now also have the flexibility to extract a second class of features
  from the semantics of the derivation.
We define a feature bracketing the most recent semantic function
  applied to each of the two child derivations; along with the function being
  applied in the rule application.
If the child is a preterminal, the semantics of the preterminal are used;
  otherwise, the outermost (most recent) function to be applied to the
  derivation is used.
% (example)
To illustrate, a tree fragment combining \te{August} and \te{2013} into
  \te{August 2013} would yield the feature \feat{$<$intersect, August, 2013$>$}.
This can be read as a feature for the rule applying the intersect function
  to August and 2013.
Furthermore, intersecting \te{August 2013} with the \th{12} of the month would
  yield a feature \feat{$<$intersect, intersect, \th{12}$>$}.
This can be read as applying the intersect function to a subtree which is
  the intersection of two terms, and to the \th{12} of the month.


\paragraph{Lexical Features}
% -- Two classes
The second large class of features extracted are lexicalized features.
These are primarily used for tagging phrases with preterminals;
  however, they are also relevant in incorporating cues from the yield
  of \ty{Nil} spans.
To illustrate, \tp{a week} and \tp{the week} have very different meanings,
  despite differing by only their \ty{Nil} tagged tokens.

% -- Preterminals
% (explanation)
In the first case, a feature is extracted over the \textit{value} of the
  preterminal being extracted, and the phrase it is subsuming.
As the type of the preterminal is deterministic from the value, encoding
  a feature on the type of the preterminal would be a coarser encoding of
  the same information, and is empirically not useful in this case.
% (ngrams)
Since a multi-word expression can parse to a single nonterminal, a feature
  is extracted for the entire $n$-gram in addition to features for each of the
  individual words.
% (example)
For example, the phrase \tp{of this} -- of type \ty{Nil} -- would have
  features extracted:
  \feat{$<$\ty{Nil}, \tp{of}$>$},
  \feat{$<$\ty{Nil}, \tp{this}$>$}, and
  \feat{$<$\ty{Nil}, \tp{of this}$>$}.

% -- Nil
In the second case -- absorbing \ty{Nil}-tagged spans --
  we extract features over the words under the \ty{Nil} span joined with
  the type or value of the other derivation.
As above, features are extracted for both n-grams and for each word in
  the phrase.
For example, combining \tp{of this} and \tp{week} would yield features
  \feat{$<$\tp{of}, EveryWeek$>$},
  \feat{$<$\tp{this}, EveryWeek$>$}, and
  \feat{$<$\tp{of this}, EveryWeek$>$}.

In both cases, numbers are featurized according to their order of magnitude,
  and whether they are ordinal.
Thus, the number tagged from \tp{thirty-first} would be featurized as an
  ordinal number of magnitude 2.

\paragraph{Semantic Validity}
% -- Top level is not null
Although some constraints can be imposed to help ensure that a top-level parse
  will be valid, absolute guarantees are difficult.
For instance, February 30 is never a valid date; but, it would be difficult
  to disallow any local rule in its derivation.
To mediate this, an indicator feature is extracted
  denoting whether the grounded semantics of the derivation is
  valid.

\paragraph{Nil Bias}
% -- Nil Bias
An indicator feature is extracted for each \ty{Nil} span tagged.
In part, this discourages over-generation of the type; in another part,
  it encourages \ty{Nil} spans to absorb as many adjacent words as possible.


% -- Segway
We proceed to describe our experimental setup and results.
